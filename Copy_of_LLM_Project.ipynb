{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Janhvi007/Flexible-RAG-style-system/blob/main/Copy_of_LLM_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is a complete “question-answering” setup for working with PDFs and images that may contain both digital text and scanned pages. It starts by installing the tools it needs: Poppler (to convert PDFs into images), Tesseract (for OCR), and some Python libraries for reading PDFs, doing OCR, working with text, and running a small open-source language model.\n",
        "\n",
        "The workflow looks like this:\n",
        "\n",
        "1. Unpack the data = It takes the 'data.zip' file we upload  and extracts everything into a folder.\n",
        "2. Read or OCR each file = For every page, it tries to pull the text directly. If that page has little or no text, it turns it into an image and runs OCR on it. This way we capture both real text and text hidden inside scanned images.\n",
        "3. Create a single text file = It then saves all extracted text from all files into one big file 'ocr_output.txt', adding a note of which file each part came from.\n",
        "4. Break the text into chunks=– Because a language model can’t read a huge document in one go, the script splits the text into overlapping pieces. These chunks are turned into a TF-IDF index (a way of quickly finding the chunks most related to a question).\n",
        "5. Load a model = It loads Microsoft’s Phi-2, a small but capable free language model, so all processing runs locally in Colab without needing an API key.\n",
        "6. Ask questions – There are two main ways to query:\n",
        "\n",
        "   *Semantic search- 'ask_one' finds the chunks most similar to your question, then has the model answer using only that context.\n",
        "   *Keyword search - 'ask_forced'pulls in small snippets of text that match specific words or phrases you give it, then answers strictly from those snippets.\n",
        "     Both methods will say “Not enough evidence” if the answer isn’t in the retrieved text.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FJIoJaxplB1G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAxr-eJ1b9XO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os, re, zipfile, io, sys, warnings, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "PIN_TRANSFORMERS = \"4.41.2\"\n",
        "PIN_ACCELERATE   = \"0.30.1\"\n",
        "\n",
        "\n",
        "ZIP_PATH = \"data.zip\"            # your uploaded zip file\n",
        "OUT_DIR = Path(\"/content/uploads\")\n",
        "OCR_TXT = Path(\"ocr_output.txt\")\n",
        "\n",
        "\n",
        "def _run(cmd):\n",
        "    print(f\"→ {cmd}\")\n",
        "    return subprocess.check_call(cmd, shell=True)\n",
        "\n",
        "\n",
        "_run(\"apt-get -qq update\")\n",
        "_run(\"apt-get -qq install -y poppler-utils tesseract-ocr >/dev/null\")\n",
        "\n",
        "\n",
        "_run(f\"pip -q install PyPDF2 pdf2image pytesseract scikit-learn pillow transformers=={PIN_TRANSFORMERS} accelerate=={PIN_ACCELERATE}\")\n",
        "\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "if os.path.exists(ZIP_PATH):\n",
        "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "        z.extractall(OUT_DIR)\n",
        "    print(f\" Extracted: {ZIP_PATH} → {OUT_DIR}\")\n",
        "else:\n",
        "    print(\" data.zip not found. Upload it to the Colab working directory.\")\n",
        "\n",
        "\n",
        "def ocr_pil(img: Image.Image) -> str:\n",
        "    try:\n",
        "        return pytesseract.image_to_string(img)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def pdf_text_or_ocr(pdf_path: str, dpi=220) -> str:\n",
        "    text_all = []\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        for i, page in enumerate(reader.pages):\n",
        "            t = (page.extract_text() or \"\").strip()\n",
        "            if len(t) >= 50:\n",
        "                text_all.append(t)\n",
        "            else:\n",
        "                # OCR that page only\n",
        "                pages = convert_from_path(pdf_path, dpi=dpi, first_page=i+1, last_page=i+1)\n",
        "                if pages:\n",
        "                    text_all.append(ocr_pil(pages[0]))\n",
        "    except Exception:\n",
        "        # fallback: OCR whole doc\n",
        "        try:\n",
        "            pages = convert_from_path(pdf_path, dpi=dpi)\n",
        "            for p in pages:\n",
        "                text_all.append(ocr_pil(p))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return \"\\n\".join(text_all)\n",
        "\n",
        "def ocr_any(path: Path) -> str:\n",
        "    p = str(path)\n",
        "    low = p.lower()\n",
        "    if low.endswith(\".pdf\"):\n",
        "        return pdf_text_or_ocr(p)\n",
        "    if low.endswith((\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\",\".webp\")):\n",
        "        try:\n",
        "            img = Image.open(p)\n",
        "            return ocr_pil(img)\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "files = []\n",
        "for root, dirs, fnames in os.walk(OUT_DIR):\n",
        "    for fn in fnames:\n",
        "        fp = Path(root) / fn\n",
        "        if fp.suffix.lower() in (\".pdf\",\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\",\".webp\"):\n",
        "            files.append(fp)\n",
        "\n",
        "print(f\" Found {len(files)} PDF/image files to process.\")\n",
        "with open(OCR_TXT, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, fp in enumerate(files, 1):\n",
        "        print(f\"[{i}/{len(files)}] {fp}\")\n",
        "        txt = ocr_any(fp)\n",
        "        if txt.strip():\n",
        "            f.write(f\"\\n\\n--- FILE: {fp} ---\\n{txt}\")\n",
        "\n",
        "print(\" Built ocr_output.txt\")\n",
        "\n",
        "\n",
        "try:\n",
        "    raw = OCR_TXT.read_text(encoding=\"utf-8\")\n",
        "except UnicodeDecodeError:\n",
        "    raw = OCR_TXT.read_text(encoding=\"latin1\")\n",
        "raw = re.sub(r\"\\s+\", \" \", raw).strip()\n",
        "print(\" OCR text length:\", len(raw))\n",
        "\n",
        "\n",
        "def chunk_text(t, chunk_size=2000, overlap=250):\n",
        "    out=[]; i=0\n",
        "    step = max(1, chunk_size - overlap)\n",
        "    while i < len(t):\n",
        "        out.append(t[i:i+chunk_size])\n",
        "        i += step\n",
        "    return [c for c in out if c.strip()]\n",
        "\n",
        "\n",
        "chunks = []\n",
        "vectorizer = None\n",
        "X = None\n",
        "\n",
        "def rebuild_retriever(text=None, chunk_size=2000, overlap=250):\n",
        "    \"\"\"(Re)build TF-IDF index from text (or ocr_output.txt).\"\"\"\n",
        "    global chunks, vectorizer, X\n",
        "    if text is None:\n",
        "        try:\n",
        "            t = OCR_TXT.read_text(encoding=\"utf-8\")\n",
        "        except UnicodeDecodeError:\n",
        "            t = OCR_TXT.read_text(encoding=\"latin1\")\n",
        "        t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    else:\n",
        "        t = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    chunks = chunk_text(t, chunk_size=chunk_size, overlap=overlap)\n",
        "    if not chunks:\n",
        "        vectorizer = None; X = None\n",
        "        print(\" No chunks built (is OCR empty?).\"); return\n",
        "    vectorizer = TfidfVectorizer(lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\", ngram_range=(1,2), min_df=1)\n",
        "    X = vectorizer.fit_transform(chunks)\n",
        "    print(f\" Retriever ready. Chunks: {len(chunks)}\")\n",
        "\n",
        "def retrieve(question, top_k=5):\n",
        "    \"\"\"Safe retrieval that auto-rebuilds and guards shape drift.\"\"\"\n",
        "    global chunks, vectorizer, X\n",
        "    if vectorizer is None or X is None or not chunks:\n",
        "        rebuild_retriever()\n",
        "        if vectorizer is None:\n",
        "            return []\n",
        "    if X.shape[0] != len(chunks):\n",
        "        rebuild_retriever()\n",
        "    q_vec = vectorizer.transform([question])\n",
        "    sims = cosine_similarity(q_vec, X)[0]\n",
        "    order = sims.argsort()[::-1][:min(top_k, len(chunks))]\n",
        "    return [chunks[i] for i in order]\n",
        "\n",
        "\n",
        "rebuild_retriever()\n",
        "\n",
        "\n",
        "model_name = \"microsoft/phi-2\"\n",
        "has_cuda = torch.cuda.is_available()\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_name,\n",
        "    device_map=\"auto\" if has_cuda else None,\n",
        "    torch_dtype=torch.float16 if has_cuda else None,\n",
        ")\n",
        "\n",
        "\n",
        "def ask_one(question, top_k=6, max_ctx_chars=2200, max_new_tokens=160):\n",
        "    \"\"\"\n",
        "    Retrieve semantically relevant chunks and answer from ONLY that context.\n",
        "    If not found, the model should say 'Not enough evidence'.\n",
        "    \"\"\"\n",
        "    ctx = \"\\n\".join(retrieve(question, top_k=top_k))[:max_ctx_chars]\n",
        "    if not ctx.strip():\n",
        "        return \"Not enough evidence\"\n",
        "    prompt = (\n",
        "        \"Context:\\n\" + ctx +\n",
        "        \"\\n\\nQuestion: \" + question +\n",
        "        \"\\nAnswer clearly using only the context above. If not in context, say 'Not enough evidence'.\\n\\nAnswer:\"\n",
        "    )\n",
        "    out = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        truncation=True,\n",
        "        pad_token_id=generator.tokenizer.eos_token_id,\n",
        "    )[0][\"generated_text\"]\n",
        "    ans = out.split(\"Answer:\")[-1].strip()\n",
        "    return ans or \"Not enough evidence\"\n",
        "\n",
        "\n",
        "import re\n",
        "from textwrap import shorten\n",
        "\n",
        "def _windows_by_regex(text, patterns_any=None, patterns_all=None, window=360, max_hits=30):\n",
        "    if not text: return []\n",
        "    patterns_any = patterns_any or []\n",
        "    patterns_all = patterns_all or []\n",
        "    any_res = [re.compile(p, re.I) for p in patterns_any] if patterns_any else [re.compile(r\".\")]\n",
        "    all_res = [re.compile(p, re.I) for p in patterns_all]\n",
        "\n",
        "    hits=[]\n",
        "    for rex in any_res:\n",
        "        for m in rex.finditer(text):\n",
        "            s = max(0, m.start()-window)\n",
        "            e = min(len(text), m.end()+window)\n",
        "            snip = text[s:e].replace(\"\\n\",\" \")\n",
        "            if all(ar.search(snip) for ar in all_res):\n",
        "                hits.append(snip)\n",
        "\n",
        "\n",
        "    seen=set(); uniq=[]\n",
        "    for h in hits:\n",
        "        k=h.strip()\n",
        "        if k not in seen:\n",
        "            seen.add(k); uniq.append(k)\n",
        "\n",
        "\n",
        "    def score(sn):\n",
        "        nums = re.findall(r\"\\$?\\d[\\d,\\.]*%?\", sn)\n",
        "        return (len(nums), len(sn))\n",
        "    uniq.sort(key=score, reverse=True)\n",
        "    return uniq[:max_hits]\n",
        "\n",
        "def ask_forced(question, patterns_any=None, patterns_all=None, max_ctx_chars=2200, max_snips=8, max_new_tokens=160):\n",
        "    try:\n",
        "        text = OCR_TXT.read_text(encoding=\"utf-8\")\n",
        "    except UnicodeDecodeError:\n",
        "        text = OCR_TXT.read_text(encoding=\"latin1\")\n",
        "    snips = _windows_by_regex(text, patterns_any, patterns_all, window=360, max_hits=30)\n",
        "    if not snips:\n",
        "        return \"Not enough evidence\"\n",
        "    ctx = \" \\n\\n---\\n\\n \".join(snips[:max_snips])[:max_ctx_chars]\n",
        "    prompt = (\n",
        "        \"Evidence:\\n\" + ctx +\n",
        "        \"\\n\\nQuestion: \" + question +\n",
        "        \"\\nAnswer strictly from the evidence above. If not present, say 'Not enough evidence'.\\n\\nAnswer:\"\n",
        "    )\n",
        "    out = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        truncation=True,\n",
        "        pad_token_id=generator.tokenizer.eos_token_id,\n",
        "    )[0][\"generated_text\"]\n",
        "    ans = out.split(\"Answer:\")[-1].strip()\n",
        "    return ans or \"Not enough evidence\"\n",
        "\n",
        "print(\"\\n Setup complete.\")\n",
        "print(\"Use ask_one('your question') for semantic retrieval, or ask_forced('q', patterns_any=[...]) for regex-anchored evidence.\")\n",
        "print(\"Examples:\")\n",
        "print(\" ask_one('Is the lending portfolio described as diversified or specialized?')\")\n",
        "print(\" ask_forced('What is the reported EPS?', patterns_any=[r'\\\\bEPS\\\\b', r'earnings per share'])\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I asked this question for example but it was giving answers from the entire zip file and all pdf's ."
      ],
      "metadata": {
        "id": "-MhG3iiBmfQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask_forced(\n",
        "  \"What is the weighted average yield on total investments at amortized cost?\",\n",
        "  patterns_any=[\"weighted average yield\", \"total investments\", \"amortized cost\"]\n",
        ")"
      ],
      "metadata": {
        "id": "xbr3-38neR6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Earlier, the code was extracting answers by searching through all the PDFs in the uploaded ZIP file at once.That worked fine when I didn’t care where the answer came from, but sometimes I need to ask a question about a specific PDF.To do that, I built this extra helper code.\n",
        "\n",
        "\n",
        "1.   The code imports 'os' and 'Path' from 'pathlib' for file handling. It also creates _pdf_cache, which stores text that’s already been extracted from a PDF so you don’t run OCR on it again. This saves time if you ask multiple questions from the same file.\n",
        "2. **find_pdf function – locating the file**\n",
        "You give it  the filename.It searches the extracted folder (OUT_DIR) for any PDF whose name contains that fragment.\n",
        "If there’s more than one match, it picks the shortest file path (a simple way to pick a best match).If nothing matches, it returns None.\n",
        "3.**pdf_text function** –getting the text from that PDFGiven the full PDF path, it first checks '_pdf_cache' to see if the text is already stored.\n",
        "If not, it calls your earlier 'ocr_any' function to extract the text (running OCR if needed), then stores and returns it.\n",
        "4.**windows_by_keywords – pulling small evidence windows**\n",
        "This is for “keyword mode” (no regex needed).\n",
        "You give it the PDF’s text and a list of keywords.\n",
        "It finds each keyword in the text and grabs a small snippet (default 200 characters before and after).\n",
        "It removes duplicates, keeps them in the order found, and limits the result to max_hits snippets.\n",
        "This keeps the evidence short and relevant.\n",
        "5.**ask_from_pdf –** the main function you’ll call.\n",
        "This is what you use to ask your question from a single PDF.\n",
        "\n",
        "Inputs:\n",
        "\n",
        "pdf_fragment: part of the filename to pick the right PDF.\n",
        "\n",
        "question: your natural-language question.\n",
        "\n",
        "mode: \"semantic\" or \"keywords\".\n",
        "\n",
        "keywords: required if mode=\"keywords\".\n",
        "\n",
        "Other parameters control chunk sizes, snippet sizes, number of results, etc.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Uom_FTAvmqGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "_pdf_cache = {}\n",
        "\n",
        "def find_pdf(name_fragment: str) -> Path | None:\n",
        "    frag = name_fragment.lower().strip()\n",
        "    matches = []\n",
        "    for root, _, files in os.walk(OUT_DIR):\n",
        "        for fn in files:\n",
        "            if fn.lower().endswith(\".pdf\") and frag in fn.lower():\n",
        "                matches.append(Path(root) / fn)\n",
        "    if not matches:\n",
        "        return None\n",
        "    matches.sort(key=lambda p: len(str(p)))\n",
        "    return matches[0]\n",
        "\n",
        "def pdf_text(pdf_path: Path) -> str:\n",
        "    key = str(pdf_path)\n",
        "    if key in _pdf_cache:\n",
        "        return _pdf_cache[key]\n",
        "    text = ocr_any(pdf_path)\n",
        "    _pdf_cache[key] = text\n",
        "    return text\n",
        "\n",
        "def windows_by_keywords(text: str, keywords: list[str], window=200, max_hits=8):\n",
        "    if not text or not keywords: return []\n",
        "    s = text\n",
        "    s_low = s.lower()\n",
        "    hits = []\n",
        "    for kw in keywords:\n",
        "        kw_low = kw.lower()\n",
        "        i = 0\n",
        "        while True:\n",
        "            j = s_low.find(kw_low, i)\n",
        "            if j == -1: break\n",
        "            start = max(0, j - window)\n",
        "            end   = min(len(s), j + len(kw) + window)\n",
        "            hits.append(s[start:end].replace(\"\\n\", \" \").strip())\n",
        "            i = j + len(kw)\n",
        "\n",
        "    seen, uniq = set(), []\n",
        "    for h in hits:\n",
        "        if h not in seen:\n",
        "            seen.add(h); uniq.append(h)\n",
        "    return uniq[:max_hits]\n",
        "\n",
        "def ask_from_pdf(pdf_fragment: str,\n",
        "                 question: str,\n",
        "                 mode: str = \"semantic\",\n",
        "                 keywords: list[str] | None = None,\n",
        "                 *,\n",
        "                 top_k: int = 8,\n",
        "                 chunk_size: int = 1400,\n",
        "                 overlap: int = 200,\n",
        "                 window: int = 220,\n",
        "                 max_snips: int = 6,\n",
        "                 max_ctx_chars: int = 1800,\n",
        "                 max_new_tokens: int = 200):\n",
        "\n",
        "    pdf_path = find_pdf(pdf_fragment)\n",
        "    if not pdf_path:\n",
        "        return f\"Not found: a PDF containing '{pdf_fragment}'.\"\n",
        "    text = pdf_text(pdf_path)\n",
        "    if not text.strip():\n",
        "        return f\"OCR produced no text for: {pdf_path.name}\"\n",
        "\n",
        "    if mode == \"semantic\":\n",
        "\n",
        "        rebuild_retriever(text=text, chunk_size=chunk_size, overlap=overlap)\n",
        "        return ask_one(question, top_k=top_k, max_ctx_chars=max_ctx_chars, max_new_tokens=max_new_tokens)\n",
        "\n",
        "\n",
        "    if not keywords:\n",
        "        return \"Please provide keywords for keyword mode (e.g., ['weighted average yield','amortized cost']).\"\n",
        "    snips = windows_by_keywords(text, keywords, window=window, max_hits=max_snips)\n",
        "    if not snips:\n",
        "        return \"Not enough evidence\"\n",
        "    ctx = \" \\n\\n---\\n\\n \".join(snips)[:max_ctx_chars]\n",
        "    prompt = (\n",
        "        \"Evidence:\\n\" + ctx +\n",
        "        \"\\n\\nQuestion: \" + question +\n",
        "        \"\\nAnswer strictly from the evidence above. If not present, say 'Not enough evidence'.\\n\\nAnswer:\"\n",
        "    )\n",
        "    out = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        truncation=True,\n",
        "        pad_token_id=generator.tokenizer.eos_token_id,\n",
        "    )[0][\"generated_text\"]\n",
        "    return (out.split(\"Answer:\")[-1].strip() or \"Not enough evidence\")\n"
      ],
      "metadata": {
        "id": "nZpN6wLVeaF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask_from_pdf(\n",
        "  \"Ares Capital Corporation_Earnings Call_2024-02-07\",\n",
        "  \"Summarize how the weighted average yield changed compared with prior periods.\",\n",
        "  mode=\"semantic\",\n",
        "  top_k=10,\n",
        "  max_ctx_chars=3500\n",
        ")\n"
      ],
      "metadata": {
        "id": "k2Z-2BYeebPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried this example for to see if it works with longer prompts.\n"
      ],
      "metadata": {
        "id": "P4yCXKG33mVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask_from_pdf(\n",
        "  \"Ares Capital Corporation_Earnings Call_2024-02-07_English.pdf\",\n",
        "  \"Using only the information provided in this specific earnings call transcript, provide a detailed explanation \"\n",
        "  \"of how the weighted average yield on total investments at amortized cost and on the company's debt and other \"\n",
        "  \"income-producing securities has changed compared with prior reporting periods. Include the exact percentage \"\n",
        "  \"values for each relevant period mentioned, explain the direction and magnitude of any changes, and summarize \"\n",
        "  \"any factors or market conditions that management cited as contributing to these changes.\",\n",
        "  mode=\"semantic\",\n",
        "  top_k=10,\n",
        "  max_ctx_chars=3500\n",
        ")\n"
      ],
      "metadata": {
        "id": "EbY5I6_-ekrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s what this add-on does ,it’s specifically for extracting information from charts and figures.This code explicitly avoids macOS junk files.\n",
        "Here’s the same explanation in simpler, human-friendly language:\n",
        "\n",
        "\n",
        "\n",
        "### 1) Get the tools ready\n",
        "\n",
        "It installs:\n",
        "\n",
        "* transformers (for AI models)\n",
        "* timm and pillow (for working with images)\n",
        "* pymupdf (fast way to turn PDF pages into images)\n",
        "\n",
        "\n",
        "### 2) Turn each PDF page into an image\n",
        "\n",
        "* Looks through your uploads folder for all PDF files.\n",
        "* Skips macOS junk files like __MACOSX.\n",
        "* Opens each PDF and saves every page as a PNG image.\n",
        "* Makes images twice as big (for better readability), but keeps them under 1400 pixels wide so they’re not too heavy.\n",
        "* Stores them in '/content/chart_pages'.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 3) Load two AI models for charts\n",
        "\n",
        "* **BLIP** → Looks at an image and writes a short caption, often catching titles, labels, or keywords.\n",
        "* **DePlot** → Looks at the image and tries to turn it into a table of numbers. It’s not perfect but can give useful data.\n",
        "\n",
        "\n",
        "### 4) Pick only the pages that seem like charts\n",
        "\n",
        "* The code checks BLIP’s caption for chart-related words like “chart”, “figure”, “EPS”, “revenue”, “yield”, “ratio”, etc.\n",
        "* If the caption matches, it:\n",
        "\n",
        "  1. Keeps the caption.\n",
        "  2. Runs DePlot to get a table of numbers (if possible).\n",
        "\n",
        "### 5) Save the chart info and add it to your search\n",
        "\n",
        "* Everything found is written to 'charts_extracted.txt', including:\n",
        "\n",
        "  1. The chart’s file name\n",
        "  2.The BLIP caption\n",
        "  3.Any table text from DePlot\n",
        "\n",
        "\n",
        " This chart text is combined with your regular OCR text 'ocr_output.txt' and the retriever is rebuilt so now searches include chart info too.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3gmInlII3u1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!pip -q install \"transformers>=4.41.0\" pillow timm==0.9.16\n",
        "!pip -q install pymupdf\n",
        "\n",
        "import os, io, re, math\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import fitz  # PyMuPDF\n",
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration\n",
        "\n",
        "CHART_IMG_DIR = Path(\"/content/chart_pages\")\n",
        "CHART_TXT = Path(\"charts_extracted.txt\")\n",
        "CHART_IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "dev_str = \"cuda:0\" if device == 0 else \"cpu\"\n",
        "print(\"Using device:\", dev_str)\n",
        "\n",
        "\n",
        "def is_sidecar(p: Path):\n",
        "    return \"/__MACOSX/\" in str(p) or p.name.startswith(\"._\") or p.name == \".DS_Store\"\n",
        "\n",
        "def render_pdf_pages_to_images(root=\"/content/uploads\", max_width=1400):\n",
        "    saved = []\n",
        "    pdfs = []\n",
        "    for r, d, fns in os.walk(root):\n",
        "        for fn in fns:\n",
        "            p = Path(r) / fn\n",
        "            if p.suffix.lower() == \".pdf\" and not is_sidecar(p):\n",
        "                try:\n",
        "                    if p.stat().st_size >= 2_000:\n",
        "                        pdfs.append(p)\n",
        "                except Exception:\n",
        "                    continue\n",
        "    pdfs.sort()\n",
        "    for pdf in pdfs:\n",
        "        try:\n",
        "            doc = fitz.open(str(pdf))\n",
        "        except Exception:\n",
        "            continue\n",
        "        for i in range(len(doc)):\n",
        "            page = doc[i]\n",
        "\n",
        "            zoom = 2.0\n",
        "            mat = fitz.Matrix(zoom, zoom)\n",
        "            pix = page.get_pixmap(matrix=mat, alpha=False)\n",
        "            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "\n",
        "            if img.width > max_width:\n",
        "                ratio = max_width / img.width\n",
        "                img = img.resize((max_width, int(img.height * ratio)), Image.LANCZOS)\n",
        "            out_path = CHART_IMG_DIR / f\"{pdf.stem}_p{i+1}.png\"\n",
        "            img.save(out_path, \"PNG\", optimize=True)\n",
        "            saved.append(out_path)\n",
        "        doc.close()\n",
        "    print(f\" Rendered {len(saved)} page images.\")\n",
        "    return saved\n",
        "\n",
        "\n",
        "blip_name = \"Salesforce/blip-image-captioning-base\"\n",
        "blip_processor = BlipProcessor.from_pretrained(blip_name)\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(blip_name).to(dev_str)\n",
        "\n",
        "deplot_name = \"google/deplot\"\n",
        "deplot_processor = Pix2StructProcessor.from_pretrained(deplot_name)\n",
        "deplot_model = Pix2StructForConditionalGeneration.from_pretrained(deplot_name).to(dev_str)\n",
        "\n",
        "def caption_image(img: Image.Image, max_new_tokens=64):\n",
        "    inputs = blip_processor(images=img, return_tensors=\"pt\").to(blip_model.device)\n",
        "    out = blip_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    return blip_processor.decode(out[0], skip_special_tokens=True).strip()\n",
        "\n",
        "def chart_to_table_text(img: Image.Image, max_new_tokens=256):\n",
        "\n",
        "    prompt = \"Generate the data table of the figure below:\"\n",
        "    inputs = deplot_processor(images=img, text=prompt, return_tensors=\"pt\").to(deplot_model.device)\n",
        "    out = deplot_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    txt = deplot_processor.decode(out[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    return txt\n",
        "\n",
        "\n",
        "def looks_like_chart(text_caption: str):\n",
        "\n",
        "    keys = [\n",
        "        \"chart\", \"figure\", \"stock\", \"price\", \"volume\", \"eps\", \"surprise\",\n",
        "        \"trend\", \"normalized\", \"revenue\", \"mm\", \"vs.\", \"index\", \"yield\",\n",
        "        \"spread\", \"LTV\", \"loan-to-value\", \"nonaccrual\", \"coverage\", \"ratio\"\n",
        "    ]\n",
        "    t = text_caption.lower()\n",
        "    return any(k in t for k in keys)\n",
        "\n",
        "def process_charts(max_pages=120):\n",
        "    page_imgs = sorted(list(CHART_IMG_DIR.glob(\"*.png\")))\n",
        "    if not page_imgs:\n",
        "        page_imgs = render_pdf_pages_to_images()\n",
        "    results = []\n",
        "    for i, p in enumerate(page_imgs[:max_pages], 1):\n",
        "        try:\n",
        "            img = Image.open(p).convert(\"RGB\")\n",
        "        except Exception:\n",
        "            continue\n",
        "        cap = caption_image(img)\n",
        "        if looks_like_chart(cap):\n",
        "            try:\n",
        "                table_txt = chart_to_table_text(img)\n",
        "            except Exception:\n",
        "                table_txt = \"\"\n",
        "            results.append((p.name, cap, table_txt))\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Processed {i}/{min(len(page_imgs), max_pages)} pages...\")\n",
        "    return results\n",
        "\n",
        "\n",
        "chart_items = process_charts(max_pages=250)\n",
        "\n",
        "with open(CHART_TXT, \"w\", encoding=\"utf-8\") as f:\n",
        "    for name, cap, table in chart_items:\n",
        "        f.write(f\"\\n\\n--- CHART_PAGE: {name} ---\\n\")\n",
        "        f.write(f\"[Caption]\\n{cap}\\n\")\n",
        "        if table and len(table.strip()) > 0:\n",
        "            f.write(\"\\n[DePlot table]\\n\")\n",
        "            f.write(table)\n",
        "\n",
        "print(f\" Charts processed: {len(chart_items)}\")\n",
        "print(f\" Saved chart text to: {CHART_TXT}\")\n",
        "\n",
        "# Merge into retriever\n",
        "try:\n",
        "    base_text = Path(\"ocr_output.txt\").read_text(encoding=\"utf-8\")\n",
        "except UnicodeDecodeError:\n",
        "    base_text = Path(\"ocr_output.txt\").read_text(encoding=\"latin1\")\n",
        "\n",
        "try:\n",
        "    chart_text = CHART_TXT.read_text(encoding=\"utf-8\")\n",
        "except UnicodeDecodeError:\n",
        "    chart_text = CHART_TXT.read_text(encoding=\"latin1\")\n",
        "\n",
        "merged = (base_text + \"\\n\\n\" + chart_text).strip()\n",
        "rebuild_retriever(text=merged, chunk_size=2200, overlap=300)\n",
        "print(\" Retriever rebuilt with chart captions + DePlot tables.\")\n",
        "print(\"Now ask chart questions via ask_one(...).\")\n"
      ],
      "metadata": {
        "id": "jrKdb_1kevBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s what it does step by step:\n",
        "\n",
        "1) Get the most relevant text for your question\n",
        "\n",
        "Uses the retrieve() function to grab the top_k chunks from the document that are most related to your question.Joins them into one text block, but limits it to max_ctx_chars characters so it’s not too big.\n",
        "\n",
        "2)  Keep only lines that have certain keywords\n",
        "\n",
        "If require_keyword_in_line=True and you provide a keywords list, it will only keep the lines from that text that contain at least one of those keywords.This helps focus on exactly the part of the text you care about.\n",
        "\n",
        "3) Decide what kinds of numbers to look for.If you don’t give your own regex patterns, it defaults to two patterns:\n",
        "\n",
        "-Percentages\n",
        "\n",
        "-Plain numbers\n",
        "\n",
        "4) Search for matching numbers\n",
        "\n",
        "Runs each regex pattern on the text and collects all matches into a list.\n",
        "\n",
        "5) Return the first match\n",
        "\n",
        "If it found any numbers, it returns:\n",
        "\n",
        "answer=  the first number found.\n",
        "\n",
        "evidence =the list of all matches found.\n",
        "\n",
        "raw_context = the chunk of text where it found them.\n",
        "\n",
        "If no numbers were found, it returns \"Not enough evidence\"."
      ],
      "metadata": {
        "id": "Kal1h8kzAQiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_number(question, keywords=None, number_regexes=None, top_k=5, max_ctx_chars=2000, require_keyword_in_line=False):\n",
        "    \"\"\"\n",
        "    Extracts numeric answer from context using regex, optionally filtering lines by keyword.\n",
        "    \"\"\"\n",
        "\n",
        "    ctx_chunks = retrieve(question, top_k=top_k)\n",
        "    context = \"\\n\".join(ctx_chunks)[:max_ctx_chars]\n",
        "\n",
        "\n",
        "    lines = context.split(\"\\n\")\n",
        "    if require_keyword_in_line and keywords:\n",
        "        lines = [line for line in lines if any(kw.lower() in line.lower() for kw in keywords)]\n",
        "        context = \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "    if number_regexes is None:\n",
        "        number_regexes = [\n",
        "            r'\\b\\d+(?:\\.\\d+)?%\\b',  # percentages\n",
        "            r'\\b\\d+(?:\\.\\d+)?\\b'    # plain numbers\n",
        "        ]\n",
        "\n",
        "    import re\n",
        "    matches = []\n",
        "    for pattern in number_regexes:\n",
        "        matches.extend(re.findall(pattern, context))\n",
        "\n",
        "\n",
        "    if matches:\n",
        "        return {\"answer\": matches[0], \"evidence\": matches, \"raw_context\": context}\n",
        "\n",
        "    return {\"answer\": \"Not enough evidence\", \"evidence\": [], \"raw_context\": context}\n",
        "\n"
      ],
      "metadata": {
        "id": "1TbjfjOig_Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask_number(\n",
        "    \"What was the weighted average LTV of all new investments this quarter?\",\n",
        "    keywords=[\"weighted average LTV\", \"LTV\", \"loan-to-value\"],\n",
        "    require_keyword_in_line=True\n",
        ")"
      ],
      "metadata": {
        "id": "HPe7r3eGhDrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer to the second task of developing a prompt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cX4yhFaPAry5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask_from_pdf(\n",
        "  \"Ares Capital Corporation_Earnings Call_2024-02-07_English.pdf\",\n",
        "  \"For the Ares Capital earnings call on February 7, 2024, check whether the lending portfolio is described as diversified or specialized. Include the strongest evidence, such as the number of portfolio companies, mix of industries or sectors, geographic reach, top-10 investment share, average deal size, and any mention of first/second-lien or unitranche loans. Summarize all the reasons management gives for following this approach, explain the benefits they mention, and note any performance results they connect to it, such as returns, yield, non-accrual rates, credit ratings, or EBITDA growth. Also point out any changes from previous periods and why they happened.\",\n",
        "  mode=\"semantic\",\n",
        "  top_k=12,\n",
        "  max_ctx_chars=5000\n",
        ")\n"
      ],
      "metadata": {
        "id": "i1HLJX0Sh8ec"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}